{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "#df_spark.write.parquet(\"sas_data\")\n",
    "#df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "#This cleaning process was achieved using Spark due to its fast, distributed data processing abilities - especially for large datasets.\n",
    "#Input Files:\n",
    "#cities\n",
    "#immigrations\n",
    "\n",
    "\n",
    "\n",
    "def model_data():\n",
    "    \"\"\"\n",
    "        @description:\n",
    "            This function performs the data cleaning processes using spark. \n",
    "            The cleaned data is then loaded to S3 (bucket specified in the config.cfg)\n",
    "        @params:\n",
    "            None.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    sc = spark_session.sparkContext\n",
    "    sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", config['AWS']['KEY'] )\n",
    "    sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", config['AWS']['SECRET'] )\n",
    "    sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "    sc._jsc.hadoopConfiguration().set(\"parquet.enable.summary-metadata\", \"false\")\n",
    "\n",
    "    IMMIGRATION_DATA  = 'datasets/immigration_data/*.parquet';\n",
    "    immigration_df = spark_session.read.format('parquet').load(IMMIGRATION_DATA, inferSchema=True , header=True);\n",
    "    CITIES_DATA = 'datasets/us-cities-demographics.csv';\n",
    "    cities_df = spark_session.read.format('csv').load(CITIES_DATA, sep=\";\", inferSchema=True , header=True);\n",
    "\n",
    "    cleaned_immigration_df = clean_immigration_data(immigration_df)\n",
    "    cleaned_cities_data = clean_cities_data(cities_df)\n",
    "\n",
    "    print(f'========================================= WRITING staging_immigrations_table TABLE TO S3 =========================================')\n",
    "    cleaned_immigration_df.repartition(1).write.mode('overwrite').parquet(f\"s3a://{config['S3']['BUCKET']}/{config['S3']['IMMIGRATION_KEY']}\")\n",
    "\n",
    "    print(f'========================================= WRITING staging_cities_table TABLE TO S3 =========================================')\n",
    "    cleaned_cities_data.repartition(1).write.mode('overwrite').parquet(f\"s3a://{config['S3']['BUCKET']}/{config['S3']['CITIES_KEY']}\")\n",
    "\n",
    "    return 'Done'\n",
    "\n",
    "\n",
    "tables = ['staging_immigrations_table', 'staging_cities_table', ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here\n",
    "\n",
    "#Both datasets are compiled and staged first to an S3 bucket in parquet file format. This is done to have a Data Lake representation of both datasets/tables, \n",
    "#and easy access to redshift. Two major tools are used in this case: Spark - for loading the data, and Amazon S3. Having these datasets stored in an S3 bucket \n",
    "#allows an opportunity to use the Data Lake method.\n",
    "#Amazons Redshift tool is used as a warehouse to store these datasets in separate facts and dimensional tables. The S3 to Redshift workflow is managed \n",
    "#Airflow which is DAG based Data Engineering workflow management system. Airflow is used in this case to ensure each of the above processes are carried \n",
    "#out in the right order, and the right scheduled time, making the ETL process as seamless as possible.\n",
    "\n",
    "\n",
    "def __init__(\n",
    "                 self,\n",
    "                redshift_conn_id = \"redshift_conn_id\",\n",
    "                aws_connection_id = \"aws_conn_id\",\n",
    "                table = \"\",\n",
    "                s3_bucket = \"\",\n",
    "                s3_key = \"\",\n",
    "                ignore_headers = 1,\n",
    "                delimeter = \",\",\n",
    "                data_format = \"csv\",\n",
    "                *args, **kwargs\n",
    "                ):\n",
    "        super(StageTablesOperator, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.redshift_conn_id = redshift_conn_id\n",
    "        self.aws_connection_id = aws_connection_id\n",
    "        self.table = table\n",
    "        self.s3_bucket = s3_bucket\n",
    "        self.s3_key = s3_key\n",
    "        self.ignore_headers = ignore_headers\n",
    "        self.delimeter = delimeter\n",
    "        self.data_format = data_format\n",
    "\n",
    "def execute(self, context):\n",
    "        self.log.info(\"Fetching credentials\")\n",
    "        aws_hook = AwsHook(self.aws_connection_id, client_type='s3')\n",
    "        aws_credentials = aws_hook.get_credentials()\n",
    "\n",
    "        redshift_conn = PostgresHook(postgres_conn_id=self.redshift_conn_id)\n",
    "        rendered_key = self.s3_key.format(**context)\n",
    "        s3_bucket_uri = f\"s3://{self.s3_bucket}/{rendered_key}\"\n",
    "    \n",
    "        formatted_sql = f\"\"\" \n",
    "                COPY {self.table}\n",
    "                FROM '{s3_bucket_uri}/'\n",
    "                ACCESS_KEY_ID '{aws_credentials.access_key}'\n",
    "                SECRET_ACCESS_KEY '{aws_credentials.secret_key}'\n",
    "                FORMAT AS {self.data_format}\n",
    "            \"\"\"\n",
    "\n",
    "        self.log.info(f\"Copying {self.table} data from s3 to redshift\")\n",
    "        redshift_conn.run(formatted_sql)\n",
    "        return 'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "#Null Value Checks\n",
    "#Data Count Checks\n",
    "#Data-types Checks\n",
    "\n",
    "\n",
    "\n",
    "def data_count_check(*args, **kwargs):\n",
    "    \"\"\"\n",
    "        description: This function runs a data count quality check on the specified table in redshift. \n",
    "            It compares the resulting number of rows with the expected value and throws an error if these values do not match.\n",
    "        params:\n",
    "            table (STR): The Redshift table needed to be tested.\n",
    "            expected_row_count (INT): The number of rows expected to be in the specified table\n",
    "        returns:\n",
    "            ValueError (Error): if the number of rows do not match.\n",
    "    \"\"\"\n",
    "    table = kwargs[\"params\"][\"table\"]\n",
    "    expected_row_count = kwargs[\"params\"][\"expected_row_count\"]\n",
    "    redshift_hook = PostgresHook(\"redshift_conn_id\")\n",
    "    records = redshift_hook.get_records(f\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM {table}\n",
    "    \"\"\")\n",
    "\n",
    "    if records[0][0] < expected_row_count:\n",
    "        raise ValueError(f\"Expected row count for {table} to be {expected_row_count}, found {records[0][0]}\");\n",
    "\n",
    "    logging.info(f\"Data count check passed with number of records = \", records[0][0])\n",
    "\n",
    "def null_value_check(*args, **kwargs):\n",
    "    \"\"\"\n",
    "        description:\n",
    "            This function runs a null value quality check on the specified table, and column in redshift. \n",
    "            It checks if the specified column has a null value in it.\n",
    "        params:\n",
    "            table (STR): The Redshift table needed to be tested.\n",
    "            column_name (STR): The name of the column to check\n",
    "        returns:\n",
    "            ValueError (Error): If at least one null value is found in the column\n",
    "    \"\"\"\n",
    "    table = kwargs[\"params\"][\"table\"]\n",
    "    column_name = kwargs[\"params\"][\"column_name\"]\n",
    "    redshift_hook = PostgresHook(\"redshift_conn_id\")\n",
    "    records = redshift_hook.get_records(f\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM {table}\n",
    "        WHERE {column_name} IS NULL\n",
    "    \"\"\")\n",
    "\n",
    "    if records[0][0] > 0:\n",
    "        raise ValueError(f\"Expected null value count for {table} to be 0, found {records[0][0]}\");\n",
    "\n",
    "    logging.info(f\"Data count check passed no null values!\")\n",
    "\n",
    "def column_type_check(*args, **kwargs):\n",
    "    \"\"\"\n",
    "        description:\n",
    "            This function runs a column type quality check on the specified table in redshift. \n",
    "            This ensures columns are of the expected data types.\n",
    "        params:\n",
    "            table (STR): The Redshift table needed to be tested.\n",
    "            column_name (STR): The name of the column to check\n",
    "            data_type (STR): Expected data type of the column. E.g. DATE, VARCHAR, INTEGER\n",
    "        returns:\n",
    "            TypeError (Error): if the columns datatype do not match\n",
    "    \"\"\"\n",
    "    table = kwargs[\"params\"][\"table\"]\n",
    "    column_name = kwargs[\"params\"][\"column_name\"]\n",
    "    data_type = kwargs[\"params\"][\"data_type\"]\n",
    "    redshift_hook = PostgresHook(\"redshift_conn_id\")\n",
    "    records = redshift_hook.get_records(f\"\"\"\n",
    "        select \"column\", type\n",
    "        from pg_table_def\n",
    "        where tablename = '{table}'\n",
    "    \"\"\")\n",
    "    for index in range(len(records)):\n",
    "        if records[index] == column_name:\n",
    "            if records[index][0] != data_type:\n",
    "                raise TypeError(f\"Invalid column data type. Expected {data_type}, found {records[index][0]}\")\n",
    "\n",
    "\n",
    "    logging.info(f\"Records\", records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
